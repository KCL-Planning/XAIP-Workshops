<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <title>XAIP-Workshops by KCL-Planning</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
      
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">XAIP Workshops</h1>
      <a href="https://kcl-planning.github.io/XAIP-Workshops/" class="btn">Home</a>
      <a href="https://kcl-planning.github.io/XAIP-Workshops/related_work" class="btn">Related Work</a>
      <a href="https://kcl-planning.github.io/XAIP-Workshops/contact" class="btn">Contact</a>
      <a href="https://github.com/KCL-Planning/XAIP-Workshops/" class="btn">View on GitHub</a>

    </section>

    <section class="main-content">
      <p>Explainable Artificial Intelligence (XAI) concerns the challenge of shedding light on opaque models where transparency is important, e.g. in analysis or synthesis tasks. In particular, as AI is increasingly being adopted for deployed applications, the challenge of supporting interaction with humans is becoming more apparent. Partly this is to support collaborations with humans but also it is a necessary step in the process of building trust as humans migrate greater competence and responsibility to AI systems. The challenge is to find effective ways to characterize and communicate the foundations of AI-driven behavior when the algorithms that drive it are far from transparent to humans. While XAI at large is primarily concerned with learning-based approaches, model-based <strong>Explainable AI Planning (XAIP)</strong> can play an important role in addressing complex decision-making procedures.</p>
      <p></p>
      <h3>ICAPS Workshops</h3>
      <p></p>
     
      <ul><li>
      <b>2019 ICAPS XAIP Workshop</b><br><br>
        <ul>
          <li><b>Mission:</b><br><font size="-1">Explainable Artificial Intelligence (XAI) concerns the challenge of shedding light on opaque models in contexts for which transparency is important, i.e. where these models could be used to solve analysis or synthesis tasks. In particular, as AI is increasingly being adopted into application solutions, the challenge of supporting interaction with humans is becoming more apparent. Partly this is to support integrated working styles, in which humans and intelligent systems cooperate in problem-solving, but also it is a necessary step in the process of building trust as humans migrate greater competence and responsibility to such systems. The challenge is to find effective ways to characterise, and to communicate, the foundations of AI-driven behaviour, when the algorithms that drive it are far from transparent to humans. While XAI at large is primarily concerned with learning-based approaches, model-based approaches are well suited -- arguably better suited -- for explanation, and Explainable AI Planning (XAIP) can play an important role in addressing complex decision-making procedures. <br><br>
            After the success of previous workshops on XAI and XAIP, the mission of this workshop is to mature and broaden the XAIP community, fostering continued exchange on XAIP topics at ICAPS.</font></li><br>
          <li><b>Topics</b> (including but not limited to):<br><font size="-1"><ul>
            <li>Frameworks for defining meaningful explanations in planning and scheduling contexts;</li>
            <li>Representation, organization, and memory content used in explanation;</li>
            <li>The creation of such content during plan generation or understanding;</li>
            <li>Generation and evaluation of explanations;</li>
            <li>The explanation process, i.e. the way in which explanations are communicated to humans (e.g., plan summaries, answers to questions);</li>
            <li>The role of knowledge and learning in explainable planners;</li>
            <li>Human vs AI models in explanations;</li>
            <li>Links between explainable planning and other disciplines (e.g. social science, argumentation);</li>
            <li>Model differences and model reconciliation;</li>
            <li>Goal reasoning and plan explanations;</li>
            <li>Excuse generation, unsolvability and explanations;</li>
            <li>Use cases and applications of explainable planning.</li>
            </ul></font></li><br>
          <li><b>Important Dates:</b><br>For submission information please refer to the workshop <a href="https://icaps19.icaps-conference.org/workshops/XAIP/index.html">webpage</a>.<font size="-1"><ul>
            <li>Paper submission: <b>March 22, 2019 (UTC-12)</b></li>
            <li>Notification: April 19, 2019</li>
            <li>Camera-ready submission: May 31, 2019</li>
            <li>Date of Workshop: July 11, 2019</li>
            </ul></font></li><br>
          <li><b>Invited Speaker:</b><br><font size="-1"><b>TBD</b></font></li><br>
          <li><b>Program:</b><br><font size="-1">
            <b>TBD</b> <br>
            </font></li><br>
          <li><b>Proceedings:</b><br><font size="-1">The proceedings can be found <a href="https://nms.kcl.ac.uk/daniele.magazzeni/XAIP18_proceedings.pdf">here</a>.</font></li><br>
          <li><b>Organizing Chairs:</b><br><font size="-1"><ul>
            <li>Tathagata Chakraborti (IBM Research AI, USA)</li>
            <li>Dustin Dannenhauer (Naval Research Laboratory, USA)</li>
            <li>Joerg Hoffmann (Saarland University, Germany)</li>
            <li>Daniele Magazzeni (King's College London, UK)</li>
            </ul></font></li><br>
          <li><b>Program Committee:</b> TBD</li>
        </ul>
      <br>See the workshop <a href="https://icaps19.icaps-conference.org/workshops/XAIP/index.html">webpage</a> for more information.
      </li></ul>
   
   
      
      <ul><li>
      <b>2018 ICAPS XAIP Workshop</b><br><br>
        <ul>
          <li><b>Mission:</b><br><font size="-1"> As AI is increasingly being adopted into application solutions, the challenge of supporting interaction with humans is becoming more apparent. Partly this is to support integrated working styles, in which humans and intelligent systems cooperate in problem-solving, but it is also a necessary step in the process of building trust as humans invest greater authority and responsibility in intelligent systems. Explainability poses challenges for many types of AI systems, including planning and scheduling (PS) systems. For example, how should a PS system justify that a plan or schedule is correct, or good, or respects supplied preferences? How can the PS system explain particular steps, ordering decisions, or resource choices? How can a PS system explain that no solution is possible, or what relaxations of the constraints would allow a solution? How can a PS system respond to questions like “what is the hard part?” or “why is this taking so long?”. These are all difficult questions that can require analysis of plan or schedule structure, analysis of the goals, constraints, and preferences, and potentially hypothetical reasoning.</font></li><br>
          <li><b>Topics</b> (including but not limited to):<br><font size="-1"><ul>
            <li>representation, organization, and knowledge needed for explanation;</li>
            <li>the creation of such content during plan generation and understanding;</li>
            <li>generation and evaluation of explanations;</li>
            <li>the way in which explanations are communicated to humans (e.g., plan summaries, answers to questions);</li>
            <li>the role of knowledge and learning in explainable planners;</li>
            <li>human vs AI models in explanations;</li>
            <li>links between explainable planning and other disciplines (e.g., social science, argumentation);</li>
            <li>use cases and applications of explainable planning</li>
            </ul></font></li><br>
          <li><b>Important Dates:</b><br><font size="-1"><ul>
            <li>Paper submission: April 6, 2018</li>
            <li>Notification: April 26, 2018</li>
            <li>Camera-ready submission: May 25, 2018</li>
            <li>Date of Workshop: June 25, 2018</li>
            </ul></font></li><br>
          <li><b>Invited Speaker:</b><br><br><font size="-1"><b>David Aha</b>, Naval Research Laboratory<br><br>
            Relating XAI (Explainable AI) to XAIP (XAI Planning)<br><br>
            <i>The DARPA Explainable AI (XAI) program is a high-profile effort, among many, whose objective is to encourage research on AI systems whose models and decisions are more accessible and transparent to users. Yet the common focus of DARPA XAI's 11 projects is machine learning; it could have been called XML rather than XAI. Still, it is raising awareness that AI researchers need to collaborate with social scientists, and others, on the design and evaluation of XAI systems. This also applies broadly to other XAI efforts, including those of interest to the ICAPS community. In this talk, I'll summarize the objectives and status of DARPA XAI, emphasizing some topics of interest to XAIP. I'll also discuss/relate some work on XAIP that has appeared at the IJCAI-17 XAI Workshop, or will appear at the upcoming IJCAI/ECAI-18 XAI Workshop, which has a broad XAI focus (i.e., not limited to ML).</i></font></li><br>
          <li><b>Program:</b><br><font size="-1">
            <b>9:00 </b> &nbsp Welcome and Introduction<br>
            <b>9:10 </b> &nbsp Invited Talk: Relating XAI (Explainable AI) to XAIP (Explainable Planning), <i>David Aha.</i><br>
            <b>10:10</b> &nbsp Human-Aware Planning Revisited: A Tale of Three Models, <i>Tathagata Chakraborti, Sarath Sreedharan, and Subbarao Kambhampati.</i><br>
            <b>10:30</b> &nbsp Coffee Break<br>
            <b>11:00</b> &nbsp Explaining Rebel Behavior in Goal Reasoning Agents, <i>Dustin Dannenhauer, Michael Floyd, Daniele Magazzeni, and David Aha.</i><br>
            <b>11:20</b> &nbsp Action Selection for Transparent Planning, <i>Aleck Macnally, Nir Lipovetzky, Miquel Ramírez, and Adrian Pearce.</i><br>
            <b>11:40</b> &nbsp Moral Permissibility of Action Plans, <i>Felix Lindner, Robert Mattmüller, and Bernhard Nebel.</i><br>
            <b>12:00</b> &nbsp Explaining Agents Plans with Valuings, <i>Michael Winikoff, Virginia Dignum, and Frank Dignum</i><br>
            <b>12:20</b> &nbsp Lunch<br>
            <b>13:40</b> &nbsp Explicability as Minimizing Distance from Expected Behavior, <i>Anagha Kulkarni, Yu Zhang, Tathagata Chakraborti, and Subbarao Kambhampati.</i><br>
            <b>14:00</b> &nbsp Generating Explanations for Mathematical Optimisation: Solution Framework and Case Study, <i>Christina Burt, Katerina Klimova, and Bernhard Primas.</i><br>
            <b>14:20</b> &nbsp What was I planning to do?, <i>Mark Roberts, Isaac Monteath, Raymond Sheh, David Aha, Piyabutra Jampathom, Keith Akins, Eric Sydow, Vikas Shivashankar, and Claude Sammut.</i><br>
            <b>14:40</b> &nbsp Plan Explanation Through Search in an Abstract Model Space: Extended Results, <i>Sarath Sreedharan, Midhun Pookkottil Madhusoodanan, Siddharth Srivastava, and Subbarao Kambhampati</i><br>
            <b>15:00</b> &nbsp Coffee Break<br>
            <b>15:30</b> &nbsp Challenges in Explainable Planning for Space Operations, <i>Simone Fratini and Nicola Policella</i><br>
            <b>15:50</b> &nbsp Improving Explanation and Effectiveness of Interactions among Autonomous Vehicles and Pedestrians, <i>Sara Manzoni, Simone Fontana, Andrea Gorrini, Domenico G. Sorrenti, and Stefania Bandini</i><br>
            <b>16:10</b> &nbsp Visualizations for an Explainable Planning Agent, <i>Tathagata Chakraborti, Kshitij Fadnis, Kartik Talamadupula, Mishal Dholakia, Biplav Srivastava, Jeffrey O. Kephart, and Rachel K. E. Bellamy</i><br>
            <b>16:30</b> &nbsp Towards Explanation-Supportive Knowledge Engineering for Planning, <i>Mauro Vallati, Lee Mccluskey, and Lukas Chrpa</i><br>
            </font></li><br>
          <li><b>Proceedings:</b><br><font size="-1">The proceedings can be found <a href="https://nms.kcl.ac.uk/daniele.magazzeni/XAIP18_proceedings.pdf">here</a>.</font></li><br>
          <li><b>Organizing Chairs:</b><br><font size="-1"><ul>
            <li>Susanne Biundo, Ulm University</li>
            <li>Pat Langley, University of Auckland</li>
            <li>Daniele Magazzeni, King’s College London</li>
            <li>David Smith</li>
          </ul></font></li><br>
          <li><b>Program Committee:</b><br><font size="-1"><ul>
            <li>Susanne Biundo (University of Ulm)</li>
            <li>John Bresina (NASA)</li>
            <li>Tathagata Chakraborti (Arizona State University)</li>
            <li>Dustin	Dannenhauer (Naval Research Laboratory)</li>
            <li>Jeremy Frank (NASA)</li>
            <li>Pat Langley (University of Auckland)</li>
            <li>Daniele Magazzeni (King’s College London)</li>
            <li>Matthew Molineaux (Knexus Research Corporation)</li>
            <li>Mark Roberts (Naval Research Laboratory)</li>
            <li>David Smith</li>
            <li>Siddharth Srivastava	(ASU)</li>
            <li>Kartik	Talamadupula (IBM)</li>
          </font></li><br>
        </ul>
      <br>See the workshop <a href="http://icaps18.icaps-conference.org/xaip/index.html">webpage</a> for more information.
      </li></ul>
      
      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/KCL-Planning/XAIP-Workshops">XAIP Workshops</a> is maintained by <a href="https://github.com/KCL-Planning">KCL-Planning</a>.</span>
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>
    </section>
  </body>
</html>
